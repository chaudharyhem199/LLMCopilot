{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Study_material\\\\Production Copilot\\\\LLMCopilot'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf_file(data='Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 1028\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chaud\\AppData\\Local\\Temp\\ipykernel_25108\\2279459995.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
      "C:\\Users\\chaud\\AppData\\Roaming\\Python\\Python310\\site-packages\\pydot\\core.py:17: UserWarning: `pydot` could not import `dot_parser`, so `pydot` will be unable to parse DOT files. The error was:  No module named 'pyparsing'\n",
      "  warnings.warn(\n",
      "c:\\Users\\chaud\\.conda\\envs\\llmcopilot\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Error motor\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables\n",
    "load_dotenv()\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "OLLAMA_API_KEY = os.environ.get('OLLAMA_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"llmcopilotlama\"\n",
    "\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\", \n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the chunks into Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings, \n",
    ")\n",
    "\n",
    "# Load existing Pinecone Index\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is error motor X?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='02b738f8-2724-4fa5-88b6-a550f0ab467a', metadata={'page': 0.0, 'source': 'Data\\\\Descriptions of errors.pdf'}, page_content='Error Motor X  \\nA failure in the width control system would mean that the machine is unable to correctly \\nadjust for the size of the PCB, which could lead to improper alignment during inspection, \\nand potentially, errors in the X or Y motor as the camera cannot correctly mo ve over the \\nPCB if it is not properly positioned.  \\nThis error typically relates to an issue with the motor responsible for moving the inspection'),\n",
       " Document(id='df37c6b1-c7e3-4d1d-b133-849d6beee4a6', metadata={'page': 0.0, 'source': 'Data\\\\Descriptions of errors.pdf'}, page_content='Error Motor X  \\nA failure in the width control system would mean that the machine is unable to correctly \\nadjust for the size of the PCB, which could lead to improper alignment during inspection, \\nand potentially, errors in the X or Y motor as the camera cannot correctly mo ve over the \\nPCB if it is not properly positioned.  \\nThis error typically relates to an issue with the motor responsible for moving the inspection'),\n",
       " Document(id='7cc10a3b-8aef-49b1-8e38-45ae1fdc5566', metadata={'page': 0.0, 'source': 'Data\\\\Descriptions of errors.pdf'}, page_content='- Update or reset the software settings if necessary to match the operational parameters of \\nthe PCB currently being processed.  \\n- Ensure proper functioning of the motion control PCB.  \\n- Reset the motion controller.    \\nError Motor Y  \\nLike Error Motor X, but on the Y -axis. It indicates a problem with the motor or controller \\nmoving the inspection head along the Y -axis.  \\n    - Perform the same checks as for Motor X.  \\n    - Make sure all connections and sensors are functioning properly.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Ollama model to answer questions\n",
    "class OllamaClient:\n",
    "    def __init__(self, base_url, api_key):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            'Authorization': f'Bearer {api_key}',\n",
    "            'Content-Type': 'application/json',\n",
    "        }\n",
    "\n",
    "    def chat(self, model, prompt, temperature=0.4, max_tokens=500):\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "        response = requests.post(f'{self.base_url}/v1/chat', headers=self.headers, data=json.dumps(payload))\n",
    "        return response\n",
    "\n",
    "# Initialize Ollama Client\n",
    "ollama_client = OllamaClient(base_url=\"http://workstation.ferienakademie.de:11434\", api_key=OLLAMA_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# System Prompt for Troubleshooting Production Line Machines\n",
    "system_prompt = (\n",
    "    \"You are an AI assistant specialized in troubleshooting production line machines. Your task is to help the user resolve any issues they're experiencing with the production line. \"\n",
    "    \"Based on the user's input, logs, and context, please: 1. Identify the problem and its potential causes. 2. Provide a clear, step-by-step solution for the user to follow. 3. Explain your reasoning, referencing specific parts of the logs or context that informed your diagnosis. 4. If applicable, mention any protocols or documentation sources you used. 5. Ask the user to confirm if the solution works, and offer to explore alternative solutions if needed.\"\n",
    "    \"Remember to be clear, concise, short, and user-friendly in your explanations. If you need more information to diagnose the problem accurately, ask the user specific questions. Don't assume a problem if the user didn't mention it. In this case, explain how you can help the user.\"\n",
    "    \"When answering, use context provided by the tools available to you. When making statements, always reference the source, i.e., the data returned by the tools. Do not make statements unless you have a tool response to back them up. You may call multiple tools at once.\"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prompt for question\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Query\n",
    "question = \"Tell me about Error motor X and how can I solve it?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have no actual context, just provide an empty string\n",
    "context_data = \"\"  # Set this to actual context data if available, or keep it as empty\n",
    "\n",
    "# Generate prompt by filling in the context\n",
    "prompt_filled = prompt.format(input=question, context=context_data)\n",
    "\n",
    "# Send request to Ollama model\n",
    "response = ollama_client.chat(model=\"llama3.1\", prompt=prompt_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [404]>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmcopilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
